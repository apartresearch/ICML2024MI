<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>NeurIPS 2023 Mechanistic Interpretability Workshop</title>

    <!-- Setup all meta-information like description and titles -->
    <meta
      name="description"
      content="The Workshop on Mechanistic Interpretability seeks to explore and drive discussions on the latest advances in interpretable machine learning models. We invite submissions of
      research, technological breakthroughs and demonstrations, as well as proposals for technical
      discussions, to be held during the workshop."
    />
    <meta
      name="keywords"
      content="NeurIPS, Mechanistic Interpretability, Workshop"
    />
    <meta name="author" content="NeurIPS 2023 Mechanistic Interpretability" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <!-- Load fonts Gothic A1 -->
    <link
      href="https://fonts.googleapis.com/css?family=Gothic+A1:400,700&display=swap"
      rel="stylesheet"
    />

    <!-- Load style.css -->
    <link rel="stylesheet" href="style.css" />
  </head>
  <body>
    <!-- Header with a background color filling approx. 300px and that has a title of the workshop and the date as a byline -->
    <header>
      <h1 class="fade-in">NeurIPS Mechanistic Interpretability Workshop</h1>
      <p class="fade-in">December 11th, 2023</p>
    </header>
    <!-- Content on white background with sections Overview, Schedule, Speakers and Organizing Committee -->
    <main class="fade-in">
      <section>

        <h2>Introduction</h2>
        <p>Even though ever larger and more capable machine learning models are being
        deployed in real-world settings, we still know concerningly little about how large neural networks
        implement many seemingly impressive capabilities. This in turn can make it difficult to reason
        about or address cases where said models exhibit undesirable behavior. One emerging approach for
        understanding the internals of neural networks is mechanistic interpretability: reverse engineering
        the algorithms implemented by neural networks into human-understandable mechanisms, often by
        examining the weights and activations of neural networks to identify circuits [Cammarata et al., 2020,
        Elhage et al., 2021] that implement particular behaviors.</p>
    
        <p>In the past two years, the subfield has seen rapid progress, despite its inherent challenge. For example,
        researchers have used newly developed mechanistic interpretability techniques to recover how large
        language models implement particular behaviors [Geiger et al., 2021, Wang et al., 2022, Olsson et al.,
        2022, Geva et al., 2023, Hanna et al., 2023] and explain various puzzles such as double descent
        [Henighan et al., 2023], scaling laws [Michaud et al., 2023], and grokking [Nanda et al., 2023,
        Chughtai et al., 2023]. At the same time, neuroscientists have found parallels between the neurons
        recovered from vision models using mechanistic interpretability [Cammarata et al.,
        2020] and neurons in mice [Ding et al., 2023] and primates [Willeke et al., 2023].</p>
    
        <p>Despite this progress, significant amounts of mechanistic interpretability work still occur in relative
        disparate circles – in particular, there seem to be relatively separate threads of work in industry and
        academia that each use their own (slightly different) notation and terminology.</p>
    
        <p>This workshop aims to bring together researchers from both industry and academia to discuss recent
        progress, address the challenges faced by this field, and clarify future goals, use cases, and agendas.
        We believe that this workshop can help foster a rich dialogue between researchers with a wide variety
        of backgrounds and ideas, which in turn will help researchers develop a deeper understanding of how
        machine learning systems work in practice.</p>
    
        <h2>Topics of discussion will include:</h2>
        <ul>
            <li>What exactly do we mean when we talk about “features" that neural networks represent?
            What is the appropriate level of analysis for understanding model internals?</li>
            <li>Many recent papers have suggested different metrics and techniques for validating mechanistic interpretations [Cammarata et al., 2020, Geiger et al., 2021, Wang et al., 2022, Chan
            et al., 2022]. What are the advantages and disadvantages of these metrics, and which
            metrics should the field use going forward? How do we avoid spurious explanations or
            “interpretability illusions" [Bolukbasi et al., 2021]?</li>
            <li>Neural networks seem to represent more features in superposition [Elhage et al., 2022,
            Gurnee et al., 2023] than they have dimensions, which poses a significant challenge for
            identifying what features particular subcomponents are representing. How much of a
            challenge does superposition pose for various approaches to mechanistic interpretability?
            What are approaches that allow us to address or circumvent this challenge?</li>
            <li>Techniques from mechanistic interpretability have been used to identify and edit behavior
            inside of neural networks [Meng et al., 2022, Turner et al., 2023]. However, other recent
            work has suggested that these edits often have unintended side effects, especially on larger
            models [Hoelscher-Obermaier et al., 2023]. How can we refine localization and editing
            behavior in more specific and scalable methods?</li>
            <li>There are several related techniques on how to corrupt internal activations inside neural
            networks to study their internal structure, such as interchange interventions [Geiger et al.,
            2021], causal tracing [Meng et al., 2022], path patching [Wang et al., 2022, Goldowsky-Dill
            et al., 2023], and causal scrubbing [Chan et al., 2022]. How are these techniques related,
            what trade-offs do they make, and what techniques should the field use going forward?</li>
            <li>Many approaches for generating mechanistic explanations are very labor intensive, leading
            to interest in automated mechanistic interpretability Foote et al. [2023], Bills et al. [2023],
            Conmy et al. [2023]. How can we develop more scalable, efficient techniques for interpreting
            ever larger and more complicated models?</li>
            <li>A significant contributor to the rapid growth of the field is the availability of introductory materials, open-sourced code, and easy-to-use software packages (for example, Nanda [2022]
            or Lindner et al. [2023]), which makes it easier for new researchers to begin to contribute
            to the field. How can the field continue to foster this beginner-friendly environment going
            forward?</li>
            <li>Mechanistic interpretability is sometimes analogized to the neuroscience of machine learning
            models. How tight is this analogy, and what can the two fields learn from each other?</li>
        </ul>
    
        <p>Besides panel discussions, invited talks, and a poster session, we also plan on running a hands-on
        tutorial exploring newer results in the field on Nanda [2022]’s TransformerLens package.</p>
      </section>
      <section>
        <h2>Invited Speakers</h2>
        <div class="speakers">
          <div class="speaker">
            <img src="img/chrisolah.jpeg" alt="Speaker" />
            <div>
              <h3><a href="https://colah.github.io/about.html">Chris Olah</a></h3>
              <p>Anthropic</p>
            </div>
          </div>
          <div class="speaker">
            <img src="img/yonathanbelinkov.jpg" alt="Speaker" />
            <div>
              <h3><a href="https://belinkov.com/">Yonathan Belinkov</a></h3>
              <p>Technion IIT</p>
            </div>
          </div>
          <div class="speaker">
            <img src="img/jacobsteinhardt.png" alt="Speaker" />
            <div>
              <h3><a href="https://jsteinhardt.stat.berkeley.edu/">Jacob Steinhardt</a></h3>
              <p>UC Berkeley</p>
            </div>
          </div>
          <div class="speaker">
            <img src="img/davidbau.jpeg" alt="Speaker" />
            <div>
              <h3><a href="https://www.khoury.northeastern.edu/people/david-bau/">David Bau</a></h3>
              <p>Northeastern University</p>
            </div>
          </div>
          <div class="speaker">
            <img src="img/stellabiderman.jpeg" alt="Speaker" />
            <div>
              <h3><a href="https://www.stellabiderman.com/">Stella Biderman</a></h3>
              <p>EleutherAI</p>
            </div>
          </div>
          <div class="speaker">
            <img src="img/naomisaphra.jpeg" alt="Speaker" />
            <div>
              <h3><a href="https://nsaphra.net/">Naomi Saphra</a></h3>
              <p>NYU</p>
            </div>
          </div>
        </div>
      </section>
      <section>
        <h2>Application process</h2>
        <p>Evaluation of submissions will be based on the originality and novelty, the technical strength, and
          relevance to the workshop topics. Notifications of acceptance will be sent to applicants by email in
          September/November 2023.</p>
      </section>
      <section>
        <h2>Submissions</h2>
        <p>We are inviting submission of 4-page papers outlining new research and technical
          demonstrations for consideration for presentation during the workshop. We also welcome proposals
          for technical discussions to be held during the workshop.
          </p>
      </section>
      <section>
        <h2>Workshop Schedule</h2>
    <ul>
        <li>09:00-09:30 - Registration and Introductory Remarks</li>
        <li>09:30-10:30 - Invited Speakers 1 & 2</li>
        <li>10:30-10:45 - Coffee Break</li>
        <li>10:45-11:45 - Invited Speakers 3 & 4</li>
        <li>11:45-12:45 - Lunch</li>
        <li>12:45-01:45 - Panel Discussion</li>
        <li>01:45-02:45 - Hands-on Tutorial 1</li>
        <li>02:45-03:00 - Coffee Break</li>
        <li>03:00-04:00 - Invited Speakers 5 & 6</li>
        <li>04:00-05:00 - Contributed Talks (3 minute/10 minute )</li>
        <li>05:00-06:00 - Poster Session and Concluding Remarks</li>
        <li>06:00       - Drinks, Chat and Social</li>
    </ul>

      </section>
      <section>
        <h2>Organizing Committee</h2>
        <div class="organizers">
          <div class="Organizer">
            <img src="img/fazlbarez.jpeg" alt="Speaker" />
            <div>
              <h3><a href="https://fbarez.github.io/">Fazl Barez</a></h3>
              <p>PhD Student Edinburgh/Oxford University</p>
            </div>
          </div>
          <div class="Organizer">
            <img src="img/morgeva.jpeg" alt="Organizer" />
            <div>
              <h3><a href="https://mega002.github.io/">Mor Geva</a></h3>
              <p>Post-Doc Google</p>
            </div>
          </div>
          <div class="Organizer">
            <img src="img/lawrencechan.jpeg" alt="Organizer" />
            <div>
              <h3><a href="https://chanlawrence.me/">Lawrence Chan</a></h3>
              <p>PhD student UC Berkeley / Alignment Research Center</p>
            </div>
          </div>
          <div class="Organizer">
            <img src="img/kayoyin.jpeg" alt="Organizer" />
            <div>
              <h3><a href="https://kayoyin.github.io/">Kayo Yin</a></h3>
              <p>PhD student UC Berkeley</p>
            </div>
          </div>
          <div class="Organizer">
            <img src="img/neelnanda.jpeg" alt="Organizer" />
            <div>
              <h3><a href="https://www.neelnanda.io/about">Neel Nanda</a></h3>
              <p>Research Engineer DeepMind</p>
            </div>
            </div>
            <div class="Organizer">
              <img src="img/maxtegmark.webp" alt="Organizer" />
              <div>
                <h3><a href="https://physics.mit.edu/faculty/max-tegmark/">Max Tegmark</a></h3>
                <p>Professor MIT</p>
              </div>
            </div>
          </div>
        </div>
      </section>
      <section>
        <h2>Correspondence</h2>
        <p>
          <!-- Emails fazl@apartresearch.com -->
          Email: <a href="mailto:fazl@robots.ox.ac.uk">fazl@robots.ox.ac.uk</a></p>
    </main>
  </body>
</html>
